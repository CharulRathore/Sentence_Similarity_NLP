{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dataset"
      ],
      "metadata": {
        "id": "toawIEPLnJMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "1iMXl5_ka3Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "qHsxijPKyunK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id = \"embedding-data/coco_captions\"\n",
        "dataset = load_dataset(dataset_id)"
      ],
      "metadata": {
        "id": "EX9pI3YV-MP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"- The {dataset_id} dataset has {dataset['train'].num_rows} examples.\")\n",
        "print(f\"- Each example is a {type(dataset['train'][0])} with a {type(dataset['train'][0]['set'])} as value.\")\n",
        "print(f\"- Examples look like this: {dataset['train'][0]['set']}\")"
      ],
      "metadata": {
        "id": "p0HUZP_n-ebq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa764c3-9a2d-4f7e-ce28-2949be6feace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The embedding-data/coco_captions dataset has 82783 examples.\n",
            "- Each example is a <class 'dict'> with a <class 'list'> as value.\n",
            "- Examples look like this: ['A clock that blends in with the wall hangs in a bathroom. ', 'A very clean and well decorated empty bathroom', 'A bathroom with a border of butterflies and blue paint on the walls above it.', 'An angled view of a beautifully decorated bathroom.', 'A blue and white bathroom with butterfly themed wall tiles.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the examples into InputExamples. It might take around 10 seconds in Google Colab.\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "train_examples = []\n",
        "test_examples = []\n",
        "train_data = dataset['train']['set']\n",
        "n_examples = dataset['train'].num_rows\n",
        "\n",
        "for i in range(n_examples):\n",
        "  train_examples.append(InputExample(texts=[train_data[i][0], train_data[i][1], train_data[i][2]]))\n",
        "\n",
        "  if (len(train_data[i])>5):\n",
        "    test_examples.append(train_data[i][3])\n",
        "    test_examples.append(train_data[i][4])\n",
        "    test_examples.append(train_data[i][5])\n",
        "  if (len(train_data[i])<5 and len(train_data[i])>3):\n",
        "    test_examples.append(train_data[i][3])\n",
        "  if (len(train_data[i])<5 and len(train_data[i])>4):\n",
        "    test_examples.append(train_data[i][4])\n",
        "\n",
        "print(\"Size of Train Set:\",len(train_examples))\n",
        "print(\"Size of Test Set:\",len(test_examples))"
      ],
      "metadata": {
        "id": "QwB3awHW-hv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5153d771-0590-4132-faf9-5245316d0632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Train Set: 82783\n",
            "Size of Test Set: 639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"We have a {type(train_examples)} of length {len(train_examples)} containing {type(train_examples[0])}'s.\")"
      ],
      "metadata": {
        "id": "9e1pzvzn-xrN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "121c6a12-4e65-42e3-c367-24b00efd5214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have a <class 'list'> of length 82783 containing <class 'sentence_transformers.readers.InputExample.InputExample'>'s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "bawNu5uXn_Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://huggingface.co/blog/how-to-train-sentence-transformers\n",
        "#Reference: https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "# Step 1: use an existing language model\n",
        "word_embedding_model = models.Transformer('bert-base-cased')\n",
        "\n",
        "## Step 2: use a pool function over the token embeddings\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "## Join steps 1 and 2 using the modules argument\n",
        "trained_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "metadata": {
        "id": "AaxuRRLC-GJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We wrap our training dataset into a Pytorch Dataloader to shuffle examples and get batch sizes.\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
      ],
      "metadata": {
        "id": "IgMEaNjB-zzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions for training a Sentence Transformers model\n",
        "from sentence_transformers import losses\n",
        "train_loss = losses.TripletLoss(model=trained_model)"
      ],
      "metadata": {
        "id": "l42Fmtkb_AWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #---------------------------TRAINING---------------------------------------\n",
        "num_epochs = 10\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data"
      ],
      "metadata": {
        "id": "m8jbZKEJ_EPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training takes around 45 minutes with a Google Colab Pro account\n",
        "trained_model.fit(train_objectives=[(train_dataloader, train_loss)],epochs=num_epochs,warmup_steps=warmup_steps) "
      ],
      "metadata": {
        "id": "XNSwM1Fd_LI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model to HF hub"
      ],
      "metadata": {
        "id": "mSM4ASZBQOLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "trained_model.save('saved_model/my_model') "
      ],
      "metadata": {
        "id": "MKGJO0BqOzXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this token:  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"
      ],
      "metadata": {
        "id": "qrr_lLx3iLwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "PW0ozhR8Oeed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    create_pr=1,\n",
        "    repo_id=\"Charul1223/bert-base-cased_coco_caption\",\n",
        "    folder_path=\"saved_model/my_model\",\n",
        "    repo_type=\"model\",\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "fn7wcoj3N_oX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}